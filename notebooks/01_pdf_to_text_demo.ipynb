{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PDF to Markdown Processing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates how to use the document processing pipeline to convert PDF documents to markdown format.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path to import document processing package\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\".\"))))\n",
    "\n",
    "# Import document processing modules\n",
    "from doc_processing.config import get_settings, ensure_directories_exist\n",
    "from doc_processing.document_pipeline import DocumentPipeline\n",
    "\n",
    "# %%\n",
    "# Check if OpenAI API key is configured\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set the 'OPENAI_API_KEY' environment variable.\")\n",
    "else:\n",
    "    print(\"OpenAI API key found.\")\n",
    "\n",
    "# %%\n",
    "# Define input and output directories\n",
    "settings = get_settings()\n",
    "ensure_directories_exist()\n",
    "\n",
    "print(f\"Input PDF directory: {settings.PDF_INPUT_DIR}\")\n",
    "print(f\"Output Markdown directory: {settings.MARKDOWN_OUTPUT_DIR}\")\n",
    "\n",
    "# %%\n",
    "# Check if input directory contains PDFs\n",
    "pdf_files = list(Path(settings.PDF_INPUT_DIR).glob('*.pdf'))\n",
    "print(f\"Found {len(pdf_files)} PDF files in input directory:\")\n",
    "for pdf_file in pdf_files[:5]:  # Show first 5 files\n",
    "    print(f\"- {pdf_file.name}\")\n",
    "if len(pdf_files) > 5:\n",
    "    print(f\"... and {len(pdf_files) - 5} more\")\n",
    "\n",
    "# %%\n",
    "# Configure pipeline for PDF to markdown conversion\n",
    "# Define custom configuration\n",
    "pipeline_config = {\n",
    "    'pdf_loader_config': {\n",
    "        'extract_metadata': True,\n",
    "        'check_password': True,\n",
    "    },\n",
    "    'pdf_processor_config': {\n",
    "        'model': 'gpt-4o',\n",
    "        'max_tokens': 1500,\n",
    "        'max_retries': 3,\n",
    "        'concurrent_pages': 2,  # Process 2 pages concurrently\n",
    "        'resolution_scale': 2,  # Higher resolution for better OCR\n",
    "        'prompt_template': 'pdf_extraction.j2',\n",
    "        'preserve_page_boundaries': True,\n",
    "    },\n",
    "    'markdown_transformer_config': {\n",
    "        'markdown_template': 'markdown_template.j2',\n",
    "        'generate_toc': True,\n",
    "        'detect_headings': True,\n",
    "        'extract_metadata': True,\n",
    "        'output_path': settings.MARKDOWN_OUTPUT_DIR,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = DocumentPipeline(pipeline_config)\n",
    "pipeline.configure_pdf_to_markdown_pipeline()\n",
    "\n",
    "print(\"Pipeline configured for PDF to Markdown conversion\")\n",
    "\n",
    "# %%\n",
    "# Process a single PDF file\n",
    "def process_single_pdf(file_path):\n",
    "    \"\"\"Process a single PDF file and return the result.\"\"\"\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    try:\n",
    "        result = pipeline.process_document(file_path)\n",
    "        print(f\"Processing complete. Output saved to: {settings.MARKDOWN_OUTPUT_DIR}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Select the first PDF file to process\n",
    "if pdf_files:\n",
    "    sample_pdf = pdf_files[0]\n",
    "    result = process_single_pdf(sample_pdf)\n",
    "    \n",
    "    # Show metadata\n",
    "    if result and 'metadata' in result:\n",
    "        print(\"\\nDocument Metadata:\")\n",
    "        for key, value in result['metadata'].items():\n",
    "            if key not in ['content', 'chunks', 'pages']:\n",
    "                print(f\"- {key}: {value}\")\n",
    "else:\n",
    "    print(\"No PDF files found to process\")\n",
    "\n",
    "# %%\n",
    "# View the first part of the extracted markdown content\n",
    "if result and 'markdown' in result:\n",
    "    markdown_content = result['markdown']\n",
    "    print(\"\\nExtracted Markdown Content (first 1000 characters):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(markdown_content[:1000] + \"...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Show output file path\n",
    "    if 'metadata' in result and 'filename' in result['metadata']:\n",
    "        output_file = Path(settings.MARKDOWN_OUTPUT_DIR) / f\"{Path(result['metadata']['filename']).stem}.md\"\n",
    "        print(f\"\\nMarkdown saved to: {output_file}\")\n",
    "        if output_file.exists():\n",
    "            print(f\"File size: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "else:\n",
    "    print(\"No markdown content generated\")\n",
    "\n",
    "# %%\n",
    "# Process all PDF files in the directory\n",
    "def process_all_pdfs():\n",
    "    \"\"\"Process all PDF files in the input directory.\"\"\"\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found to process\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(pdf_files)} PDF files...\")\n",
    "    try:\n",
    "        results = pipeline.process_directory(settings.PDF_INPUT_DIR)\n",
    "        print(f\"Processing complete. {len(results)} files processed.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF directory: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment to process all PDFs\n",
    "# all_results = process_all_pdfs()\n",
    "\n",
    "# %%\n",
    "# Define a function to extract document statistics\n",
    "def get_document_stats(result):\n",
    "    \"\"\"Extract statistics from processing result.\"\"\"\n",
    "    if not result:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        'title': result.get('metadata', {}).get('title', 'Unknown'),\n",
    "        'filename': result.get('metadata', {}).get('filename', 'Unknown'),\n",
    "        'num_pages': result.get('metadata', {}).get('num_pages', 0),\n",
    "        'num_processed_pages': result.get('metadata', {}).get('num_processed_pages', 0),\n",
    "        'content_length': len(result.get('content', '')),\n",
    "        'markdown_length': len(result.get('markdown', '')),\n",
    "        'processing_status': 'Success' if 'error' not in result else f\"Error: {result['error']}\"\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Example of getting stats for the processed document\n",
    "if result:\n",
    "    stats = get_document_stats(result)\n",
    "    print(\"\\nDocument Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Custom modifications for specific documents\n",
    "def customize_markdown(markdown_content, title):\n",
    "    \"\"\"Customize markdown content with additional information.\"\"\"\n",
    "    header = f\"\"\"# {title}\n",
    "\n",
    "*Document processed with Document Processing Pipeline*\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "    footer = \"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "*End of document*\n",
    "\"\"\"\n",
    "    return header + markdown_content + footer\n",
    "\n",
    "# Example of customizing markdown\n",
    "if result and 'markdown' in result:\n",
    "    original_markdown = result['markdown']\n",
    "    title = result.get('metadata', {}).get('title', 'Untitled Document')\n",
    "    custom_markdown = customize_markdown(original_markdown, title)\n",
    "    \n",
    "    # Save to custom file\n",
    "    custom_output_file = Path(settings.MARKDOWN_OUTPUT_DIR) / f\"{Path(result['metadata']['filename']).stem}_custom.md\"\n",
    "    \n",
    "    with open(custom_output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(custom_markdown)\n",
    "    \n",
    "    print(f\"\\nCustomized markdown saved to: {custom_output_file}\")\n",
    "\n",
    "# %%\n",
    "# Function to analyze document content\n",
    "def analyze_document_content(document):\n",
    "    \"\"\"Analyze document content and extract insights.\"\"\"\n",
    "    if not document or 'content' not in document:\n",
    "        return \"No content to analyze\"\n",
    "    \n",
    "    content = document['content']\n",
    "    word_count = len(content.split())\n",
    "    \n",
    "    # Simple analysis\n",
    "    analysis = {\n",
    "        'word_count': word_count,\n",
    "        'character_count': len(content),\n",
    "        'average_word_length': sum(len(word) for word in content.split()) / max(1, word_count),\n",
    "        'number_of_paragraphs': content.count('\\n\\n') + 1,\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example of analyzing document content\n",
    "if result and 'content' in result:\n",
    "    analysis = analyze_document_content(result)\n",
    "    print(\"\\nDocument Content Analysis:\")\n",
    "    for key, value in analysis.items():\n",
    "        print(f\"- {key}: {value:.2f}\" if isinstance(value, float) else f\"- {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Extract headings to create a table of contents\n",
    "def extract_toc(markdown_content):\n",
    "    \"\"\"Extract headings from markdown to create a table of contents.\"\"\"\n",
    "    if not markdown_content:\n",
    "        return []\n",
    "    \n",
    "    toc = []\n",
    "    for line in markdown_content.split('\\n'):\n",
    "        if line.startswith('#'):\n",
    "            # Count number of # to determine heading level\n",
    "            level = 0\n",
    "            for char in line:\n",
    "                if char == '#':\n",
    "                    level += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Extract heading text\n",
    "            heading_text = line[level:].strip()\n",
    "            if heading_text:\n",
    "                toc.append((level, heading_text))\n",
    "    \n",
    "    return toc\n",
    "\n",
    "# Example of extracting TOC\n",
    "if result and 'markdown' in result:\n",
    "    toc = extract_toc(result['markdown'])\n",
    "    if toc:\n",
    "        print(\"\\nTable of Contents:\")\n",
    "        for level, heading in toc:\n",
    "            indent = \"  \" * (level - 1)\n",
    "            print(f\"{indent}- {heading}\")\n",
    "    else:\n",
    "        print(\"No headings found in the document\")\n",
    "\n",
    "# %%\n",
    "# Create a batch processing function with progress tracking\n",
    "def batch_process_with_progress(file_paths, batch_size=3):\n",
    "    \"\"\"Process files in batches with progress tracking.\"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"No files to process\")\n",
    "        return []\n",
    "    \n",
    "    total_files = len(file_paths)\n",
    "    processed_files = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Starting batch processing of {total_files} files...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch = file_paths[i:i+batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1} ({len(batch)} files)...\")\n",
    "        \n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                print(f\"Processing: {file_path.name}\")\n",
    "                result = pipeline.process_document(file_path)\n",
    "                results.append(result)\n",
    "                processed_files += 1\n",
    "                print(f\"Completed {processed_files}/{total_files} files ({processed_files/total_files*100:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "        \n",
    "    print(f\"\\nBatch processing complete. {processed_files}/{total_files} files processed successfully.\")\n",
    "    return results\n",
    "\n",
    "# Uncomment to run batch processing with progress tracking\n",
    "# batch_results = batch_process_with_progress(pdf_files[:5])  # Process first 5 files\n",
    "\n",
    "# %%\n",
    "# Function to check processing quality\n",
    "def check_processing_quality(result):\n",
    "    \"\"\"Check the quality of PDF processing.\"\"\"\n",
    "    if not result or 'content' not in result:\n",
    "        return {'quality': 'Unknown', 'issues': ['No content found']}\n",
    "    \n",
    "    content = result.get('content', '')\n",
    "    issues = []\n",
    "    \n",
    "    # Check for common issues\n",
    "    if len(content) < 100:\n",
    "        issues.append(\"Content is very short\")\n",
    "    \n",
    "    if \"�\" in content:\n",
    "        issues.append(\"Contains encoding issues (replacement characters)\")\n",
    "    \n",
    "    # Check for common OCR errors\n",
    "    ocr_error_patterns = [r'\\bl\\b', r'\\bI\\b', r'\\b0\\b', r'\\bO\\b']\n",
    "    for pattern in ocr_error_patterns:\n",
    "        if len(re.findall(pattern, content)) > 50:  # Too many occurrences might indicate OCR issues\n",
    "            issues.append(f\"Potential OCR error with pattern {pattern}\")\n",
    "    \n",
    "    # Check for layout issues\n",
    "    if '\\n\\n\\n\\n' in content:\n",
    "        issues.append(\"Contains excessive line breaks\")\n",
    "    \n",
    "    # Determine overall quality\n",
    "    if not issues:\n",
    "        quality = \"Good\"\n",
    "    elif len(issues) <= 2:\n",
    "        quality = \"Fair\"\n",
    "    else:\n",
    "        quality = \"Poor\"\n",
    "    \n",
    "    return {'quality': quality, 'issues': issues}\n",
    "\n",
    "# Example of checking processing quality\n",
    "if result:\n",
    "    import re\n",
    "    quality_check = check_processing_quality(result)\n",
    "    print(\"\\nProcessing Quality Check:\")\n",
    "    print(f\"- Overall quality: {quality_check['quality']}\")\n",
    "    if quality_check['issues']:\n",
    "        print(\"- Issues found:\")\n",
    "        for issue in quality_check['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"- No issues found\")\n",
    "\n",
    "# %%\n",
    "# Function to compare different processing configurations\n",
    "def compare_processing_configs(pdf_path):\n",
    "    \"\"\"Compare different processing configurations on the same PDF.\"\"\"\n",
    "    if not pdf_path or not Path(pdf_path).exists():\n",
    "        print(\"Invalid PDF path\")\n",
    "        return {}\n",
    "    \n",
    "    configs = {\n",
    "        'default': {\n",
    "            'pdf_processor_config': {\n",
    "                'model': 'gpt-4o',\n",
    "                'resolution_scale': 2,\n",
    "            }\n",
    "        },\n",
    "        'high_res': {\n",
    "            'pdf_processor_config': {\n",
    "                'model': 'gpt-4o',\n",
    "                'resolution_scale': 3,  # Higher resolution\n",
    "            }\n",
    "        },\n",
    "        'detailed_prompt': {\n",
    "            'pdf_processor_config': {\n",
    "                'model': 'gpt-4o',\n",
    "                'resolution_scale': 2,\n",
    "                'preserve_page_boundaries': True,\n",
    "                'extract_columns': True,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\nProcessing with configuration: {config_name}\")\n",
    "        \n",
    "        # Create pipeline with configuration\n",
    "        test_pipeline = DocumentPipeline(config)\n",
    "        test_pipeline.configure_pdf_to_text_pipeline()\n",
    "        \n",
    "        # Process document\n",
    "        try:\n",
    "            result = test_pipeline.process_document(pdf_path)\n",
    "            content_length = len(result.get('content', ''))\n",
    "            quality = check_processing_quality(result)\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'content_length': content_length,\n",
    "                'quality': quality['quality'],\n",
    "                'issues': quality['issues'],\n",
    "            }\n",
    "            \n",
    "            print(f\"- Content length: {content_length}\")\n",
    "            print(f\"- Quality: {quality['quality']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {config_name} configuration: {str(e)}\")\n",
    "            results[config_name] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to compare different configurations on a sample PDF\n",
    "# if pdf_files:\n",
    "#     comparison = compare_processing_configs(pdf_files[0])\n",
    "#     print(\"\\nConfiguration Comparison Results:\")\n",
    "#     for config_name, result in comparison.items():\n",
    "#         print(f\"\\n{config_name.upper()}:\")\n",
    "#         for key, value in result.items():\n",
    "#             if key == 'issues':\n",
    "#                 print(f\"- issues: {', '.join(value) if value else 'None'}\")\n",
    "#             else:\n",
    "#                 print(f\"- {key}: {value}\")\n",
    "\n",
    "# %%\n",
    "# Save all templates to the template directory\n",
    "def create_templates():\n",
    "    \"\"\"Create Jinja templates in the template directories.\"\"\"\n",
    "    from doc_processing.templates.prompt_manager import PromptTemplateManager\n",
    "    \n",
    "    # Create template manager\n",
    "    template_manager = PromptTemplateManager()\n",
    "    \n",
    "    # Define PDF extraction template\n",
    "    pdf_extraction_template = \"\"\"{# Template for PDF extraction with GPT-4 Vision #}\n",
    "Extract all text from this image of page {{ page_number }} of a PDF document. Follow these specific instructions:\n",
    "\n",
    "1. Extract ALL visible text from the image, including:\n",
    "   - Main body text\n",
    "   - Headers and footers\n",
    "   - Page numbers\n",
    "   - Table contents\n",
    "   - Figure captions\n",
    "   - Footnotes and endnotes\n",
    "   - Sidebar text\n",
    "   - References and citations\n",
    "\n",
    "2. Maintain the original formatting structure as closely as possible:\n",
    "   - Preserve paragraph breaks\n",
    "   - Maintain heading levels (indicated with # for Markdown)\n",
    "   - Preserve bullet points and numbered lists\n",
    "   - Keep table structure using Markdown table format when possible\n",
    "\n",
    "3. For tables:\n",
    "   - If the table is complex, try to maintain its structure using Markdown table syntax\n",
    "   - If that's not possible, describe the table structure briefly and extract all text content\n",
    "\n",
    "4. For mathematical equations and formulas:\n",
    "   - Render using LaTeX notation inside $ $ delimiters when possible\n",
    "   - If the equation is complex, describe it briefly\n",
    "\n",
    "5. For special content:\n",
    "   - Clearly label footnotes as [Footnote: ...]\n",
    "   - Indicate page numbers as [Page X]\n",
    "   - Mark figure captions as [Figure: ...]\n",
    "\n",
    "6. Error correction:\n",
    "   - Fix obvious OCR errors\n",
    "   - Correct hyphenation issues at line breaks\n",
    "   - Join words split across lines\n",
    "\n",
    "7. Return ONLY the extracted text content.\n",
    "   - Do NOT include explanations, descriptions, or commentary about the extraction process\n",
    "   - Do NOT include placeholders for images or other non-text elements beyond brief labels\n",
    "   - Do NOT include your own analysis of the content\n",
    "\n",
    "8. Special instructions:\n",
    "{% if config.preserve_page_boundaries %}\n",
    "   - Start the extraction with \"=== Page {{ page_number }} ===\" to clearly mark page boundaries\n",
    "{% endif %}\n",
    "{% if config.extract_columns %}\n",
    "   - Handle multi-column layouts by extracting text from left to right, column by column\n",
    "{% endif %}\n",
    "{% if config.handle_rotated_text %}\n",
    "   - Extract any rotated or sideways text, noting its orientation\n",
    "{% endif %}\n",
    "{% if config.extract_tables_only %}\n",
    "   - Focus on extracting tables and their data, minimizing extraction of body text\n",
    "{% endif %}\n",
    "\n",
    "Return only the extracted text, formatted cleanly and ready for use.\"\"\"\n",
    "    \n",
    "    # Define markdown template\n",
    "    markdown_template = \"\"\"{# Template for Markdown document generation #}\n",
    "# {{ title }}\n",
    "\n",
    "{% if metadata %}\n",
    "## Document Metadata\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "{% for key, value in metadata.items() %}\n",
    "{% if key not in ['content', 'chunks', 'pages', 'error'] and value is not none %}\n",
    "| {{ key }} | {{ value }} |\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "{% endif %}\n",
    "\n",
    "{% if toc %}\n",
    "{{ toc }}\n",
    "{% endif %}\n",
    "\n",
    "{% for section in sections %}\n",
    "{% if section.heading %}\n",
    "{{'#' * section.heading.level }} {{ section.heading.text }}\n",
    "{% endif %}\n",
    "\n",
    "{{ section.content }}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "---\n",
    "\n",
    "*Document processed on {{ now().strftime('%Y-%m-%d %H:%M:%S') }}*\"\"\"\n",
    "    \n",
    "    # Create templates\n",
    "    try:\n",
    "        # Create prompt template\n",
    "        template_manager.create_prompt_template(\"pdf_extraction.j2\", pdf_extraction_template)\n",
    "        print(\"PDF extraction template created\")\n",
    "        \n",
    "        # Create output template\n",
    "        template_manager.create_output_template(\"markdown_template.j2\", markdown_template)\n",
    "        print(\"Markdown template created\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating templates: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to create templates\n",
    "# create_templates()\n",
    "\n",
    "# %%\n",
    "# Final summary of the notebook\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: PDF to Markdown Processing Pipeline Demo\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis notebook demonstrated how to:\")\n",
    "print(\"1. Configure and set up a document processing pipeline\")\n",
    "print(\"2. Process PDF documents to extract text content\")\n",
    "print(\"3. Convert text to structured Markdown format\")\n",
    "print(\"4. Analyze document content and quality\")\n",
    "print(\"5. Customize output formats and templates\")\n",
    "print(\"\\nNext steps could include:\")\n",
    "print(\"- Integrating with Weaviate vector database\")\n",
    "print(\"- Adding support for other document types\")\n",
    "print(\"- Implementing batch processing with progress tracking\")\n",
    "print(\"- Creating a web interface for document processing\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unified",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
